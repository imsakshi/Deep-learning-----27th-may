{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Today we will learn about a special category of NN known as Recurrent Neural Networks.\n",
    "Recurrent  : Anything that repeats itself.\n",
    "NN: You already know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNs are specifically useful in NLP related datasets as they can identify long term and short patterns \n",
    "in the dataset and also we can use RNN for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i lived in japan for 20 years now i am india but i know how to frequently speak ....... ?\n",
    "japnese --- ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic Convention:\n",
    "1. ANN : Continous data\n",
    "2. CNN : Image and video data\n",
    "3. RNN : Textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A RNN is made up of recurrent Neurons,like how normal neural networks are made up of a normal neurons.\n",
    "If we can understand the working of a recurrent neuron then obviously it will be easier to understand RNN\n",
    "as network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A recurrent neuron resembles very much to a normal neuron, except it also has a connection \n",
    "pointing backwards.\n",
    "At each time stamp(t) which is also called frame, this recurrent neuron receives an input\n",
    "denoted by X_t as well as its output from the previous timestamp X_t-1\n",
    "Since there is no output for the first time stamp at the beginning we set the output\n",
    "of the previous state as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In Normal Neuron,\n",
    "   No. of features(Columns) ==== no of elements in the weight vector \"w\"\n",
    "   tumhare data ka input --------- \"w\" naam ka ek weight vector.\n",
    "In recurrent Neuron,\n",
    "   No. of features(Columns) ==== no of elements in weight vetor \"w\"\n",
    "    tumhare data ka input ------ \"w\" naam ka ek weight vector + past-time ka\n",
    "                                    output ka input vector.\n",
    "\n",
    "   kyunki is baar do weight vectors h , so dono ko \"w\" nhi bol sakte , tum khud hee \n",
    "   confuse ho haoge, ek ka naam w_x or w_y.\n",
    "For every recurrent neuron ,we provide two sets of weight .\n",
    "  1. one for the input x_t.\n",
    "  2. other for the output of previous state y_(t-1)\n",
    "Ques>  Timestamp (t-2) ka output kya hoga? ---- y_(t-2)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The major advantage of RNN is that its equation can be given at\n",
    "any time -stamp.\n",
    "y_t = f(x_t , y_(t-1))\n",
    "y_t-1 = f(x_(t-1) , y_(t-2))\n",
    "y_t-2 = f(x_(t-2) , y_(t-3))\n",
    "y_t-3 = f(x_(t-3) , y_(t-4))\n",
    "In English,\n",
    " we can say that y_t is a function of x_t and y_(t) which is again a \n",
    "    function of x(t-1) and y(t-2) and so on...\n",
    " \"y_t is a function of all the inputs since time t = 0\"                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.SimpleRNN(32,input_shape = [4, ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ab maine aapko btaya ki RNNs suffers from the problem of\n",
    "vanishing gradient infact it is their biggest limitation\n",
    "so can u pause urself for 2 min n again look at the architecture\n",
    "of RNN and try to think why this problem occurs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
