{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Today we will learn about a special category of NN known as Recurrent Neural Networks.\n",
    "Recurrent  : Anything that repeats itself.\n",
    "NN: You already know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNs are specifically useful in NLP related datasets as they can identify long term and short patterns \n",
    "in the dataset and also we can use RNN for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i lived in japan for 20 years now i am india but i know how to frequently speak ....... ?\n",
    "japnese --- ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic Convention:\n",
    "1. ANN : Continous data\n",
    "2. CNN : Image and video data\n",
    "3. RNN : Textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A RNN is made up of recurrent Neurons,like how normal neural networks are made up of a normal neurons.\n",
    "If we can understand the working of a recurrent neuron then obviously it will be easier to understand RNN\n",
    "as network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A recurrent neuron resembles very much to a normal neuron, except it also has a connection \n",
    "pointing backwards.\n",
    "At each time stamp(t) which is also called frame, this recurrent neuron receives an input\n",
    "denoted by X_t as well as its output from the previous timestamp X_t-1\n",
    "Since there is no output for the first time stamp at the beginning we set the output\n",
    "of the previous state as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In Normal Neuron,\n",
    "   No. of features(Columns) ==== no of elements in the weight vector \"w\"\n",
    "   tumhare data ka input --------- \"w\" naam ka ek weight vector.\n",
    "In recurrent Neuron,\n",
    "   No. of features(Columns) ==== no of elements in weight vetor \"w\"\n",
    "    tumhare data ka input ------ \"w\" naam ka ek weight vector + past-time ka\n",
    "                                    output ka input vector.\n",
    "\n",
    "   kyunki is baar do weight vectors h , so dono ko \"w\" nhi bol sakte , tum khud hee \n",
    "   confuse ho haoge, ek ka naam w_x or w_y.\n",
    "For every recurrent neuron ,we provide two sets of weight .\n",
    "  1. one for the input x_t.\n",
    "  2. other for the output of previous state y_(t-1)\n",
    "Ques>  Timestamp (t-2) ka output kya hoga? ---- y_(t-2)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The major advantage of RNN is that its equation can be given at\n",
    "any time -stamp.\n",
    "y_t = f(x_t , y_(t-1))\n",
    "y_t-1 = f(x_(t-1) , y_(t-2))\n",
    "y_t-2 = f(x_(t-2) , y_(t-3))\n",
    "y_t-3 = f(x_(t-3) , y_(t-4))\n",
    "In English,\n",
    " we can say that y_t is a function of x_t and y_(t) which is again a \n",
    "    function of x(t-1) and y(t-2) and so on...\n",
    " \"y_t is a function of all the inputs since time t = 0\"                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.SimpleRNN(32,input_shape = [4, ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ab maine aapko btaya ki RNNs suffers from the problem of\n",
    "vanishing gradient infact it is their biggest limitation\n",
    "so can u pause urself for 2 min n again look at the architecture\n",
    "of RNN and try to think why this problem occurs?\n",
    "Now, we understood that why the problem of vanishing gradiet is so common in RNNs\n",
    "and its all because of recursive weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10:24 ---- v.g, why in rnn? ---- t-1\n",
    "10:25 ---- rnns have memory cells. --- t\n",
    "10:26 --- mam screen is not visibble --- (t+1)\n",
    "Dont you think that you have a memory cell?\n",
    "Ans: Yes.\n",
    "    Similarly RNNs have also have men=mory cells.\n",
    "This aspect gives rise to a number of memory based architecture that are used in place of\n",
    "Vanilla RNN in real-world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A single recurrent neuron is a very basic memory cell. By convention we provide every memory \n",
    "cell with a state denoted by \"h\"--- cell k state ko \"h\" bola jata h.\n",
    "At time stamp t , RNNs state will be h_(t)\n",
    "\n",
    "At any time stamp \"t\" we can denote the state of a cell by h_(t), why h? -- Hidden.\n",
    "In general context, a cells state at time h_(t) is a function of the inputs at that time-stamp\n",
    "and its previous states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Limitations of RNN:\n",
    "    1. Vanishing Gradient.\n",
    "    2. Jo humare RNNs memory cells h unki ek property ki important information or non important\n",
    "    information m farq nhi kr pate ,\n",
    "    mtl they can't predict according to the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The food here is not  good. --- negative , -- positive\n",
    "The food here is not bad.\n",
    "\n",
    "Rnns allows us sequential predictions:\n",
    "\n",
    "\n",
    "\"The colour of the sky is .......\" #Blue \n",
    "For such sentences simple Rnns works fine..\n",
    "\n",
    "I spent 20 long years working for the under-previlaged kids in spain. I then moved to africa\n",
    "but i know how to frequently speak .......  #spanish.\n",
    "\n",
    "To make a proper prediction here, RNNs needs to remember the context, This relevant information may be\n",
    "seperated from the point where it is needed , by a huge load of irrelevant data. This is where RNNs fails.\n",
    "\n",
    "Improvement over RNN : LSTM(Long short Term Memory)\n",
    "\n",
    "\n",
    "Widely used to detect long term patterns in masssive datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Look at the forget gate,\n",
    "C_(t-1) ----- long term state\n",
    "h_(t) ----- short term state\n",
    "Suppose kro C_(t-1) s jo value aa rhi h , i dont want that value in my output therefore i send zero from my first FC layer\n",
    "----- aage ka sab zero ho jayega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_t = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_t * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here we can see ki saare elements zero ho gye , mtlb hm bhul gye ki kya aa\n",
    "rha tha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_t *1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here the output remains same mtlb humne sab yaad rakha ki kya aa rha taha.\n",
    "therefore we used sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1,0,0\n",
    "0.-1,0   ----- Edge detection\n",
    "0,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if we want to save current output in a long-term state then wo kaam addition wala gate krta h..\n",
    "agr hm i(t) ki value zero kr denge then koi add nhi hoga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last wala addition gate aaapke current output ko long term state m jodna chahta h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since there are lots of computations going on, LSTMs can be overwhelming at first but remember\n",
    "under the hood it is always some computation that is getting solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How many weights are their in one neuron of RNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(t) = logistic(Wxft . X(t) + Whf_t . h_(t-1) + bf)\n",
    "g(t) = tanh(Wxg_t . X(t) + Whg_t . h_(t-1) + bg)\n",
    "i(t) = logistic(Wxi_t . X(t) + Whi_t . h_(t-1) + bi)\n",
    "o(t) = logistic(Wxo_t . X(t) + Who_t . h_(t-1) + bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "At every time stamp LSTMs perform all these computations under the hood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
