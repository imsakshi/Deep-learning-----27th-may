{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yesterday we learned about the structure of convolutional Neural Network, there we learnt about all\n",
    "convolution operations and tried it on mnist dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we will learn Various architectures of CNN.\n",
    "ab humein aage ki classes m ein most popular CNN architecture of padhna hoga and implement bhi karenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vanishing Gradient/ Exploding Gradient Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We all know that \"BackPropogation\" works by moving from output layer to the input layer and propagating gradients along\n",
    "the way. Once gradients are computed they are used to update parameters in the updation step.\n",
    "\n",
    "Sometimes, gradients get smaller and smaller as the algorithm progresses towords the lower layers.\n",
    "\n",
    "As a result the updation step doesnot really changes the weights of lower layers and the training never converges\n",
    "This is known as vanishing gradients problem.\n",
    "\n",
    "if the opoosite happens, then it is a exploding gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent kisi neuron ki value ko tweak/change krta h , or change krna mtlb ghatana yea badhana\n",
    "and bht baar NN k architecture ki wajah s gradient explode ho jata h----- mtlb NN\n",
    "k andar visfot ho jata h mtlb gradient ki value bht badi yea chooti ho jati h. and both the \n",
    "problem vanish our Model.\n",
    "1. Vanishing Gradient : It enters into infinite loop and function kbhi nhi phuchega\n",
    "    global minima pr.\n",
    "2. Exploding :   It becomes Computationally Expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Solution : We tackle this problem in number of ways:\n",
    "        1. Initialization Technique \n",
    "        2. External Normalization Technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN m weights kaise initialize hote h?\n",
    "ans: Randomly...\n",
    "But when we use initialization it will still be random but there is a bound on\n",
    "upper and lower limit. [1,100]--- suppose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "External Normalization --- aap already jaante ho , kal jab humne cnn mnis p\n",
    "lagaya tha to tmne jo each pixel value ko 255 s divide kia tha?\n",
    "so that humare saare pixels ki value will come down to the range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras library ---- Francois Challot creator of keras library.\n",
    "keras library mein different types ki intialization strategy implemented hai\n",
    "jinme s kuch k naam h:\n",
    "1. Xavier or Glorat Initialization\n",
    "2. LeCun Initialization\n",
    "3.He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanishing Gradient will make the signal die at the lower layers.\n",
    "We don't want the signal to die hence we use an initialization strategy.\n",
    "\n",
    "we want the variance of the outputs of each and every layer to be equal to the variance \n",
    "of their inputs.\n",
    "\n",
    "\n",
    "\n",
    "We also wants the gradient to have equal variances before and after passing through\n",
    "a particular hidden layer in the propagation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kuch flaw dikh rha h is proble m ?\n",
    "Here we are learning initializatin strategy --- and isko use krne s aam taur pr V.G problem solve ho jati h lekin  jab\n",
    "NN train ho rha hoga then multiple epochs m v.g might appear in later stages of training.\n",
    "\n",
    "Starting m weight update badiya rha lekin later stages m sab nan ho gya,\n",
    "\n",
    "At one time stamp NN 1 batch ko dekhta h , batch size 2 ki power m lena convention h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using an initialization technique we can significantly reduce the \n",
    "problem of vanishing gradient but it does not guareentee a solution.\n",
    "The VG problem may appear in the later stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we can again provide a solution to the aformentioned problem and that technique is known as\n",
    "batch Normalization Technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.BatchNormalization()\n",
    "keras.layers.Dense()\n",
    "keras.layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a technique used to avoid overfitting. IT basically provides a new computation parameter\n",
    "that acts as a constraint and attempts to counter overfitting.\n",
    "\n",
    "Reg is a technique so if you apply constraint to the model in any want then that way could be an\n",
    "example of Regularization.\n",
    "\n",
    "for eg: batchNormalization is also a form of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agr aapme ML mein RMSE and MAE pdha h then w=ek general formula for kth norm associated with a vector \"v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = (1,2) in l_k\n",
    "L_1 = manhatton norm , l_2 = Euclidean norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and from this concept we have developed two popular regularizers \n",
    "1. l1 regularizer\n",
    "2. L2 regularizer\n",
    "In keras the code for applying regularization is passed as an argument\n",
    "in the layer.\n",
    "keras.regularizer.l1()\n",
    "keras.regularizer.l2()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apart from l1 and l2 reg there is a popular technique known as Dropout which is heavilyused in\n",
    "CNNs, \n",
    "Geoffrey Hinton --- He is the man who introduces the concept of Dropout and backpropogation.\n",
    "\n",
    "what the Dropout mechanism does is , \" at every training step , every neuron has a probability \"p\" of  being temporarily dropped out.\"\n",
    "\n",
    "Every neuron ---- including input neuron and always exclude the output neuron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The meaning of \"dropped out \" is that the neuron will be completely ignored during a particular\n",
    "training step but it could be active in the next training step.\n",
    "\n",
    "The \"p\" hyperparameter is known as the drop out rate.\n",
    "\n",
    "Ki jb hum training step krte h , n data bhejte h batches mein usme kuch neurons ko drop kr dena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "So far in NNs we discussed\n",
    "1. initializer\n",
    "2.optimizers\n",
    "3. Loss\n",
    "4. activation\n",
    "5. Neurons\n",
    "6.weights\n",
    "7.layers\n",
    "8.epochs and batch size\n",
    "9. convolution\n",
    "10. You knw how to design CNNs\n",
    "11. Batch Normaliztion\n",
    "12. Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we have a chest  x-ry imahes of people having covid-19 and normal\n",
    "so now dimensions of real-image is different , so aaj s hum shuru krte h opencv\n",
    "jo ki ek powerful computer vision yea image processing library h.\n",
    "\n",
    "\n",
    "jab hum hard-core image processing krte h then we use opencv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
